# AI Trader Battlefield - Optimization & Architecture Transformation Plan

## Executive Summary

This document outlines the transformation of AI Trader Battlefield from a static historical simulation to a **live market simulation platform** with dynamic Finnhub data, adaptive AI algorithms, and 3-hour checkpoint-based learning cycles.

### Key Changes Overview

| Aspect | Current System | New System |
|--------|---------------|------------|
| **Data Source** | Yahoo Finance (historical CSV) | Finnhub (live streaming data) |
| **Simulation Timing** | Real-time execution | Day-delayed (uses previous day's data) |
| **Order Execution** | Order book matching | Open market (instant execution) |
| **Algorithm Updates** | Static (generated once) | Adaptive (re-evaluates every 3 hours) |
| **Evaluation Checkpoints** | End of session only | Every 3 hours + final |
| **Performance Metrics** | ROI only | ROI + Sharpe Ratio + Max Drawdown + Risk Measures |
| **Database** | Static CSV files | Dynamic PostgreSQL/MongoDB database |

---


## New Architecture Overview

```
Day N (Current Day)
    â†“
Fetch Day N-1 data from Finnhub
    â†“
Store in Dynamic Database (PostgreSQL/MongoDB)
    â†“
Distribute to AI Agents
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        24-Hour Simulation (Day N-1 replay)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Checkpoint 1 (3 hours)                         â”‚
â”‚    â”œâ”€ Compare agent simulation vs actual market â”‚
â”‚    â”œâ”€ Analyze: High, Low, Volume, VWAP, etc.    â”‚
â”‚    â”œâ”€ Calculate performance delta               â”‚
â”‚    â”œâ”€ Generate new algorithm prompt             â”‚
â”‚    â””â”€ Update algorithm for next checkpoint      â”‚
â”‚                                                  â”‚
â”‚  Checkpoint 2 (6 hours)                         â”‚
â”‚    â”œâ”€ [Same process]                            â”‚
â”‚    â””â”€ ...                                       â”‚
â”‚                                                  â”‚
â”‚  Checkpoint 3 (9 hours)                         â”‚
â”‚  Checkpoint 4 (12 hours)                        â”‚
â”‚  Checkpoint 5 (15 hours)                        â”‚
â”‚  Checkpoint 6 (18 hours)                        â”‚
â”‚  Checkpoint 7 (21 hours)                        â”‚
â”‚  Checkpoint 8 (24 hours) - FINAL                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Evaluation:
  - ROI Ranking
  - Sharpe Ratio
  - Maximum Drawdown
  - Risk-Adjusted Returns
  - Volatility Measures
    â†“
Declare Winner (Best AI Model)
```

---

## Phase 1: Database Layer Implementation

### 1.1 Database Schema Design

**Technology Choice:** PostgreSQL (relational, time-series optimized)

**Schema:**

```sql
-- Market data table (Finnhub source)
CREATE TABLE market_data (
    id SERIAL PRIMARY KEY,
    ticker VARCHAR(10) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    open DECIMAL(12, 4),
    high DECIMAL(12, 4),
    low DECIMAL(12, 4),
    close DECIMAL(12, 4),
    volume BIGINT,
    vwap DECIMAL(12, 4),  -- Volume-weighted average price
    source_date DATE NOT NULL,  -- Original market date (N-1)
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(ticker, timestamp, source_date)
);
CREATE INDEX idx_market_data_ticker_timestamp ON market_data(ticker, timestamp);
CREATE INDEX idx_market_data_source_date ON market_data(source_date);

-- Simulation sessions
CREATE TABLE simulation_sessions (
    session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    ticker VARCHAR(10) NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    source_date DATE NOT NULL,  -- Day N-1
    simulation_date DATE NOT NULL,  -- Day N (today)
    status VARCHAR(20) DEFAULT 'running',  -- running, completed, failed
    config JSONB,  -- Simulation configuration
    created_at TIMESTAMP DEFAULT NOW()
);

-- Agent performance per simulation
CREATE TABLE agent_performance (
    id SERIAL PRIMARY KEY,
    session_id UUID REFERENCES simulation_sessions(session_id),
    agent_name VARCHAR(100) NOT NULL,
    model_provider VARCHAR(50) NOT NULL,
    model_id VARCHAR(100) NOT NULL,
    initial_cash DECIMAL(12, 2),
    final_cash DECIMAL(12, 2),
    final_stock_value DECIMAL(12, 2),
    total_value DECIMAL(12, 2),
    roi DECIMAL(8, 4),  -- Return on Investment
    sharpe_ratio DECIMAL(8, 4),  -- Risk-adjusted return
    max_drawdown DECIMAL(8, 4),  -- Maximum drawdown %
    volatility DECIMAL(8, 4),  -- Portfolio volatility
    total_trades INTEGER,
    winning_trades INTEGER,
    losing_trades INTEGER,
    avg_trade_size DECIMAL(12, 2),
    rank INTEGER,  -- Final ranking
    created_at TIMESTAMP DEFAULT NOW()
);

-- Checkpoint evaluations (every 3 hours)
CREATE TABLE checkpoints (
    id SERIAL PRIMARY KEY,
    session_id UUID REFERENCES simulation_sessions(session_id),
    agent_name VARCHAR(100) NOT NULL,
    checkpoint_number INTEGER NOT NULL,  -- 1-8 (every 3 hours)
    checkpoint_time TIMESTAMP NOT NULL,
    simulated_hours INTEGER,  -- 3, 6, 9, 12, 15, 18, 21, 24

    -- Agent simulation state
    agent_portfolio_value DECIMAL(12, 2),
    agent_cash DECIMAL(12, 2),
    agent_shares INTEGER,
    agent_roi DECIMAL(8, 4),

    -- Actual market data at this checkpoint
    actual_market_high DECIMAL(12, 4),
    actual_market_low DECIMAL(12, 4),
    actual_market_close DECIMAL(12, 4),
    actual_market_volume BIGINT,
    actual_market_vwap DECIMAL(12, 4),

    -- Agent predictions vs reality
    prediction_accuracy DECIMAL(8, 4),  -- How close agent got to optimal
    performance_delta DECIMAL(8, 4),  -- Agent ROI vs market return

    -- Updated algorithm
    algorithm_version INTEGER,
    algorithm_code TEXT,  -- New algorithm after re-evaluation
    update_reason TEXT,  -- Why algorithm was updated

    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(session_id, agent_name, checkpoint_number)
);

-- Trade executions
CREATE TABLE trades (
    id SERIAL PRIMARY KEY,
    session_id UUID REFERENCES simulation_sessions(session_id),
    agent_name VARCHAR(100) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    side VARCHAR(4) NOT NULL,  -- BUY, SELL
    quantity INTEGER NOT NULL,
    price DECIMAL(12, 4) NOT NULL,
    total_value DECIMAL(12, 2),
    cash_after DECIMAL(12, 2),
    shares_after INTEGER,
    checkpoint_number INTEGER,  -- Which 3-hour period
    created_at TIMESTAMP DEFAULT NOW()
);
CREATE INDEX idx_trades_session_agent ON trades(session_id, agent_name);
```

**Alternative:** MongoDB for flexibility
```javascript
// Collections
market_data: { ticker, timestamp, ohlcv, vwap, source_date }
simulation_sessions: { session_id, ticker, dates, status, config }
agent_performance: { session_id, agent_name, metrics, final_ranking }
checkpoints: { session_id, agent_name, checkpoint_num, state, market_data, algorithm }
trades: { session_id, agent_name, timestamp, side, quantity, price }
```

### 1.2 Database Implementation Tasks

**Files to Create:**
- `backend/database/db_config.py` - Database connection configuration
- `backend/database/models.py` - SQLAlchemy ORM models
- `backend/database/market_data_repository.py` - Market data CRUD operations
- `backend/database/simulation_repository.py` - Simulation session management
- `backend/database/checkpoint_repository.py` - Checkpoint data management

**Dependencies to Add (requirements.txt):**
```
psycopg2-binary==2.9.9  # PostgreSQL adapter
SQLAlchemy==2.0.25      # ORM
alembic==1.13.1         # Database migrations
```

---

## Phase 2: Finnhub Integration

### 2.1 Finnhub API Setup

**Documentation:** https://finnhub.io/docs/api

**Key Endpoints:**
- `/stock/candle` - Historical OHLCV data
- `/quote` - Real-time quote
- `/stock/metric` - Financial metrics

**API Key:** Free tier provides:
- 60 API calls/minute
- Stock prices, financials, earnings
- Company news

### 2.2 Implementation

**File:** `backend/market/finnhub_provider.py`

```python
import finnhub
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict
from .tick_generator import TickData

class FinnhubDataProvider:
    """
    Fetches historical market data from Finnhub API
    for day N-1 (previous trading day)
    """

    def __init__(self, api_key: str):
        self.client = finnhub.Client(api_key=api_key)

    def fetch_previous_day_data(
        self,
        ticker: str,
        resolution: str = '5'  # 1, 5, 15, 30, 60, D, W, M
    ) -> List[TickData]:
        """
        Fetch data from previous trading day (Day N-1)

        Args:
            ticker: Stock symbol (AAPL, MSFT, etc.)
            resolution: Candle resolution (5 = 5-minute candles)

        Returns:
            List of TickData objects spanning 24 hours
        """
        # Calculate Day N-1
        today = datetime.now().date()
        previous_day = today - timedelta(days=1)

        # If weekend, go back to Friday
        while previous_day.weekday() >= 5:  # Saturday=5, Sunday=6
            previous_day -= timedelta(days=1)

        # Unix timestamps for Finnhub API
        start_ts = int(datetime.combine(previous_day, datetime.min.time()).timestamp())
        end_ts = int(datetime.combine(previous_day, datetime.max.time()).timestamp())

        # Fetch candle data
        response = self.client.stock_candles(
            ticker,
            resolution,
            start_ts,
            end_ts
        )

        if response['s'] != 'ok':
            raise Exception(f"Finnhub API error: {response}")

        # Convert to TickData objects
        ticks = []
        for i in range(len(response['t'])):
            tick = TickData(
                timestamp=datetime.fromtimestamp(response['t'][i]),
                open=response['o'][i],
                high=response['h'][i],
                low=response['l'][i],
                close=response['c'][i],
                volume=response['v'][i],
                symbol=ticker
            )
            ticks.append(tick)

        return ticks

    def calculate_vwap(self, ticks: List[TickData]) -> float:
        """Calculate volume-weighted average price"""
        total_volume = sum(t.volume for t in ticks)
        if total_volume == 0:
            return 0.0
        vwap = sum(t.close * t.volume for t in ticks) / total_volume
        return vwap

    def fetch_and_store(self, ticker: str, db_repo) -> int:
        """
        Fetch previous day data and store in database
        Returns number of records inserted
        """
        ticks = self.fetch_previous_day_data(ticker)
        vwap = self.calculate_vwap(ticks)

        records_inserted = 0
        for tick in ticks:
            db_repo.insert_market_data(
                ticker=ticker,
                timestamp=tick.timestamp,
                open=tick.open,
                high=tick.high,
                low=tick.low,
                close=tick.close,
                volume=tick.volume,
                vwap=vwap,
                source_date=tick.timestamp.date()
            )
            records_inserted += 1

        return records_inserted
```

**Configuration (.env):**
```bash
FINNHUB_API_KEY=your_finnhub_api_key
OPENROUTER_API_KEY=your_openrouter_api_key
DATABASE_URL=postgresql://user:password@localhost:5432/ai_trader_battlefield
```

**Dependencies to Add:**
```
finnhub-python==2.4.19
```

---

## Phase 3: Remove Order Book, Use Open Market Execution

### 3.1 Changes Required

**Current Flow (with Order Book):**
```
Agent decision â†’ Create LIMIT/MARKET order â†’ OrderBook matching â†’ Trade execution
```

**New Flow (Open Market):**
```
Agent decision (BUY/SELL/HOLD) â†’ Instant execution at current market price
```

### 3.2 Implementation

**File:** `backend/market/market_simulation.py`

**Modify `_process_tick()` method:**

```python
def _process_tick(self, tick: TickData, current_tick: int):
    """
    Process single tick - SIMPLIFIED for open market execution
    No order book, instant execution at market price
    """
    current_price = tick.close

    # Get agent decisions
    for agent_name, agent in self.agent_manager.agents.items():
        portfolio = self.agent_manager.portfolios[agent_name]

        try:
            # Get agent decision
            orders = agent.on_tick(
                price=current_price,
                current_tick=current_tick,
                cash=portfolio.cash,
                stock=portfolio.shares
            )

            # Process each order (should be BUY, SELL, or HOLD)
            for order in orders:
                side = order.get('side', 'HOLD').upper()
                quantity = order.get('quantity', 0)

                if side == 'BUY' and quantity > 0:
                    # Instant buy at current market price
                    cost = quantity * current_price
                    if portfolio.cash >= cost:
                        portfolio.cash -= cost
                        portfolio.shares += quantity

                        # Record trade
                        self._record_trade(
                            agent_name=agent_name,
                            side='BUY',
                            quantity=quantity,
                            price=current_price,
                            timestamp=tick.timestamp,
                            checkpoint_number=self._get_checkpoint_number(current_tick)
                        )

                elif side == 'SELL' and quantity > 0:
                    # Instant sell at current market price
                    if portfolio.shares >= quantity:
                        portfolio.cash += quantity * current_price
                        portfolio.shares -= quantity

                        # Record trade
                        self._record_trade(
                            agent_name=agent_name,
                            side='SELL',
                            quantity=quantity,
                            price=current_price,
                            timestamp=tick.timestamp,
                            checkpoint_number=self._get_checkpoint_number(current_tick)
                        )

        except Exception as e:
            self.logger.error(f"Agent {agent_name} error: {e}")

    # Record tick data
    self.tick_history.append({
        'tick': current_tick,
        'timestamp': tick.timestamp,
        'price': current_price,
        'volume': tick.volume
    })
```

### 3.3 Files to Modify

- **Remove/Deprecate:** `backend/market/order_book.py` (no longer needed)
- **Simplify:** `backend/market/market_simulation.py` (remove order book logic)
- **Update:** `backend/main.py` (remove order book configuration)
- **Update:** Agent implementations to return simple BUY/SELL/HOLD decisions

---

## Phase 4: 3-Hour Checkpoint System

### 4.1 Checkpoint Logic

**Checkpoint Schedule (for 24-hour simulation):**
- Checkpoint 1: 3 hours (12.5% complete)
- Checkpoint 2: 6 hours (25% complete)
- Checkpoint 3: 9 hours (37.5% complete)
- Checkpoint 4: 12 hours (50% complete)
- Checkpoint 5: 15 hours (62.5% complete)
- Checkpoint 6: 18 hours (75% complete)
- Checkpoint 7: 21 hours (87.5% complete)
- Checkpoint 8: 24 hours (100% complete - FINAL)

### 4.2 Implementation

**File:** `backend/market/checkpoint_manager.py`

```python
from typing import Dict, List
from datetime import datetime
from dataclasses import dataclass

@dataclass
class CheckpointEvaluation:
    checkpoint_number: int
    simulated_hours: int
    agent_name: str

    # Agent state
    portfolio_value: float
    cash: float
    shares: int
    roi: float

    # Market reality at this checkpoint
    actual_high: float
    actual_low: float
    actual_close: float
    actual_volume: int
    actual_vwap: float

    # Performance analysis
    prediction_accuracy: float
    performance_delta: float

    # Algorithm update
    new_algorithm_code: str
    update_reason: str


class CheckpointManager:
    """
    Manages 3-hour checkpoint evaluations and algorithm updates
    """

    def __init__(self, total_ticks: int = 288):
        # Assuming 5-minute candles: 288 ticks = 24 hours
        self.total_ticks = total_ticks
        self.checkpoint_interval = total_ticks // 8  # Every 3 hours

    def is_checkpoint(self, current_tick: int) -> bool:
        """Check if current tick is a checkpoint"""
        return current_tick > 0 and current_tick % self.checkpoint_interval == 0

    def get_checkpoint_number(self, current_tick: int) -> int:
        """Get checkpoint number (1-8)"""
        return current_tick // self.checkpoint_interval

    def evaluate_checkpoint(
        self,
        session_id: str,
        agent_name: str,
        current_tick: int,
        portfolio: 'Portfolio',
        tick_history: List[Dict],
        db_repo
    ) -> CheckpointEvaluation:
        """
        Evaluate agent performance at checkpoint
        Compare simulation vs actual market
        """
        checkpoint_num = self.get_checkpoint_number(current_tick)
        simulated_hours = checkpoint_num * 3

        # Get actual market data for this time period
        start_idx = (checkpoint_num - 1) * self.checkpoint_interval
        end_idx = current_tick
        period_ticks = tick_history[start_idx:end_idx]

        actual_high = max(t['price'] for t in period_ticks)
        actual_low = min(t['price'] for t in period_ticks)
        actual_close = period_ticks[-1]['price']
        actual_volume = sum(t['volume'] for t in period_ticks)
        actual_vwap = sum(t['price'] * t['volume'] for t in period_ticks) / actual_volume

        # Calculate agent performance
        current_price = period_ticks[-1]['price']
        portfolio_value = portfolio.get_total_value(current_price)
        roi = portfolio.get_roi(10000.0, current_price)

        # Calculate market return for comparison
        start_price = period_ticks[0]['price']
        market_return = ((actual_close - start_price) / start_price) * 100

        # Performance delta: how much better/worse than market
        performance_delta = roi - market_return

        # Prediction accuracy: how close agent got to optimal
        # Optimal = buy at low, sell at high
        optimal_return = ((actual_high - actual_low) / actual_low) * 100
        prediction_accuracy = (roi / optimal_return) * 100 if optimal_return > 0 else 0

        evaluation = CheckpointEvaluation(
            checkpoint_number=checkpoint_num,
            simulated_hours=simulated_hours,
            agent_name=agent_name,
            portfolio_value=portfolio_value,
            cash=portfolio.cash,
            shares=portfolio.shares,
            roi=roi,
            actual_high=actual_high,
            actual_low=actual_low,
            actual_close=actual_close,
            actual_volume=actual_volume,
            actual_vwap=actual_vwap,
            prediction_accuracy=prediction_accuracy,
            performance_delta=performance_delta,
            new_algorithm_code="",  # Generated below
            update_reason=""
        )

        # Store checkpoint in database
        db_repo.insert_checkpoint(session_id, evaluation)

        return evaluation
```

### 4.3 Algorithm Re-evaluation

**File:** `backend/open_router/adaptive_algo_gen.py`

```python
def generate_updated_algorithm(
    agent_name: str,
    model_id: str,
    checkpoint_eval: CheckpointEvaluation,
    ticker: str,
    openrouter_api_key: str
) -> str:
    """
    Generate updated algorithm based on checkpoint performance
    """

    # Build prompt with performance feedback
    prompt = f"""
You are a professional algorithmic trader. Your previous trading algorithm has been running for {checkpoint_eval.simulated_hours} hours.

**Performance So Far:**
- Current ROI: {checkpoint_eval.roi:.2f}%
- Portfolio Value: ${checkpoint_eval.portfolio_value:,.2f}
- Cash: ${checkpoint_eval.cash:,.2f}
- Shares Held: {checkpoint_eval.shares}
- Prediction Accuracy: {checkpoint_eval.prediction_accuracy:.2f}%
- Performance vs Market: {checkpoint_eval.performance_delta:+.2f}%

**Market Conditions (Last 3 Hours):**
- High: ${checkpoint_eval.actual_high:.2f}
- Low: ${checkpoint_eval.actual_low:.2f}
- Close: ${checkpoint_eval.actual_close:.2f}
- Volume: {checkpoint_eval.actual_volume:,}
- VWAP: ${checkpoint_eval.actual_vwap:.2f}

**Task:**
Re-evaluate your trading strategy and generate an IMPROVED algorithm for the next 3-hour period.

**Analysis Points:**
1. Did you miss opportunities (e.g., didn't buy at low, didn't sell at high)?
2. Is your strategy too conservative or too aggressive?
3. Are you correctly identifying trends vs noise?
4. Should you adjust position sizing?

**Requirements:**
- Function signature: `def execute_trade(ticker: str, cash_balance: float, shares_held: int) -> str`
- Return: "BUY", "SELL", or "HOLD"
- Use yfinance for data: `import yfinance as yf`
- Include error handling
- Optimize for better prediction accuracy

Generate the improved Python code below:
"""

    # Call OpenRouter API
    response = requests.post(
        "https://openrouter.ai/api/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json"
        },
        json={
            "model": model_id,
            "messages": [
                {"role": "system", "content": "You are an expert algorithmic trading system."},
                {"role": "user", "content": prompt}
            ]
        }
    )

    result = response.json()
    new_code = result['choices'][0]['message']['content']

    # Extract code from markdown if present
    if "```python" in new_code:
        new_code = new_code.split("```python")[1].split("```")[0].strip()
    elif "```" in new_code:
        new_code = new_code.split("```")[1].split("```")[0].strip()

    # Determine update reason
    if checkpoint_eval.performance_delta < -5:
        update_reason = "Underperforming market significantly"
    elif checkpoint_eval.prediction_accuracy < 30:
        update_reason = "Low prediction accuracy, missing opportunities"
    elif checkpoint_eval.shares == 0 and checkpoint_eval.roi < 0:
        update_reason = "Not taking positions, too conservative"
    else:
        update_reason = "Incremental optimization based on market feedback"

    return new_code, update_reason
```

---

## Phase 5: Advanced Performance Metrics

### 5.1 Metrics Implementation

**File:** `backend/market/performance_metrics.py`

```python
import numpy as np
from typing import List, Dict

class PerformanceMetrics:
    """
    Calculate advanced trading performance metrics
    """

    @staticmethod
    def calculate_roi(initial_value: float, final_value: float) -> float:
        """Return on Investment (%)"""
        return ((final_value - initial_value) / initial_value) * 100

    @staticmethod
    def calculate_sharpe_ratio(
        returns: List[float],
        risk_free_rate: float = 0.0
    ) -> float:
        """
        Sharpe Ratio: Risk-adjusted return
        Higher is better (> 1 is good, > 2 is very good)
        """
        if len(returns) < 2:
            return 0.0

        excess_returns = [r - risk_free_rate for r in returns]
        mean_return = np.mean(excess_returns)
        std_return = np.std(excess_returns)

        if std_return == 0:
            return 0.0

        sharpe = mean_return / std_return
        return sharpe

    @staticmethod
    def calculate_max_drawdown(portfolio_values: List[float]) -> float:
        """
        Maximum Drawdown: Largest peak-to-trough decline (%)
        Lower is better
        """
        if len(portfolio_values) < 2:
            return 0.0

        peak = portfolio_values[0]
        max_dd = 0.0

        for value in portfolio_values:
            if value > peak:
                peak = value
            dd = ((peak - value) / peak) * 100
            if dd > max_dd:
                max_dd = dd

        return max_dd

    @staticmethod
    def calculate_volatility(returns: List[float]) -> float:
        """
        Portfolio volatility (standard deviation of returns)
        Lower is better for risk-averse
        """
        if len(returns) < 2:
            return 0.0
        return np.std(returns)

    @staticmethod
    def calculate_sortino_ratio(
        returns: List[float],
        risk_free_rate: float = 0.0
    ) -> float:
        """
        Sortino Ratio: Like Sharpe but only penalizes downside volatility
        Higher is better
        """
        if len(returns) < 2:
            return 0.0

        excess_returns = [r - risk_free_rate for r in returns]
        mean_return = np.mean(excess_returns)

        # Downside deviation (only negative returns)
        downside_returns = [r for r in excess_returns if r < 0]
        if len(downside_returns) == 0:
            return float('inf')  # No downside

        downside_std = np.std(downside_returns)
        if downside_std == 0:
            return 0.0

        sortino = mean_return / downside_std
        return sortino

    @staticmethod
    def calculate_win_rate(trades: List[Dict]) -> float:
        """
        Percentage of profitable trades
        """
        if len(trades) == 0:
            return 0.0

        profitable = sum(1 for t in trades if t.get('profit', 0) > 0)
        return (profitable / len(trades)) * 100

    @staticmethod
    def calculate_profit_factor(trades: List[Dict]) -> float:
        """
        Ratio of gross profit to gross loss
        > 1.0 means profitable overall
        """
        gross_profit = sum(t.get('profit', 0) for t in trades if t.get('profit', 0) > 0)
        gross_loss = abs(sum(t.get('profit', 0) for t in trades if t.get('profit', 0) < 0))

        if gross_loss == 0:
            return float('inf') if gross_profit > 0 else 0.0

        return gross_profit / gross_loss
```

### 5.2 Integration into Simulation

**Modify:** `backend/market/market_simulation.py`

```python
def _generate_results(self) -> Dict:
    """
    Generate final results with advanced metrics
    """
    from .performance_metrics import PerformanceMetrics

    results = []

    for agent_name, portfolio in self.agent_manager.portfolios.items():
        # Get portfolio value history
        portfolio_values = self._get_portfolio_value_history(agent_name)

        # Calculate returns
        returns = []
        for i in range(1, len(portfolio_values)):
            ret = ((portfolio_values[i] - portfolio_values[i-1]) / portfolio_values[i-1]) * 100
            returns.append(ret)

        # Get trades
        agent_trades = [t for t in self.trade_history if t['agent'] == agent_name]

        # Calculate all metrics
        final_value = portfolio_values[-1]
        roi = PerformanceMetrics.calculate_roi(self.config.initial_cash, final_value)
        sharpe = PerformanceMetrics.calculate_sharpe_ratio(returns)
        max_dd = PerformanceMetrics.calculate_max_drawdown(portfolio_values)
        volatility = PerformanceMetrics.calculate_volatility(returns)
        sortino = PerformanceMetrics.calculate_sortino_ratio(returns)
        win_rate = PerformanceMetrics.calculate_win_rate(agent_trades)
        profit_factor = PerformanceMetrics.calculate_profit_factor(agent_trades)

        results.append({
            'agent': agent_name,
            'final_value': final_value,
            'roi': roi,
            'sharpe_ratio': sharpe,
            'max_drawdown': max_dd,
            'volatility': volatility,
            'sortino_ratio': sortino,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'total_trades': len(agent_trades)
        })

    # Sort by composite score (weighted average of metrics)
    # This is customizable based on what you value most
    for r in results:
        r['composite_score'] = (
            r['roi'] * 0.4 +           # 40% weight on ROI
            r['sharpe_ratio'] * 20 +   # Sharpe ratio scaled
            -r['max_drawdown'] * 0.2 + # Penalize drawdown
            r['win_rate'] * 0.2        # 20% weight on win rate
        )

    results.sort(key=lambda x: x['composite_score'], reverse=True)

    # Assign ranks
    for i, r in enumerate(results, 1):
        r['rank'] = i

    return {
        'leaderboard': results,
        'winner': results[0]['agent'] if results else None
    }
```

---

## Phase 6: Integration & Workflow

### 6.1 Daily Execution Flow

**File:** `backend/daily_runner.py`

```python
"""
Daily simulation runner - executed via cron job or scheduler
Runs simulation using previous day's market data
"""

import schedule
import time
from datetime import datetime
from market.finnhub_provider import FinnhubDataProvider
from database.market_data_repository import MarketDataRepository
from database.simulation_repository import SimulationRepository
from market.market_simulation import MarketSimulation
from market.checkpoint_manager import CheckpointManager
import os

def run_daily_simulation():
    """
    Main daily simulation workflow
    """
    print(f"[{datetime.now()}] Starting daily AI trader simulation...")

    # Initialize
    finnhub_key = os.getenv('FINNHUB_API_KEY')
    db_url = os.getenv('DATABASE_URL')

    finnhub = FinnhubDataProvider(finnhub_key)
    market_repo = MarketDataRepository(db_url)
    sim_repo = SimulationRepository(db_url)

    # 1. Fetch previous day data from Finnhub
    ticker = 'AAPL'  # Can be parameterized
    print(f"Fetching Day N-1 data for {ticker}...")
    records = finnhub.fetch_and_store(ticker, market_repo)
    print(f"Stored {records} market data records")

    # 2. Load AI agents (from config or database)
    selected_models = [
        'openai/gpt-4-turbo',
        'google/gemini-pro',
        'anthropic/claude-3-sonnet',
        'meta-llama/llama-3-70b',
        'deepseek/deepseek-chat',
        'mistralai/mistral-large'
    ]

    # 3. Create simulation session
    session = sim_repo.create_session(
        ticker=ticker,
        agents=selected_models,
        config={
            'checkpoint_interval_hours': 3,
            'total_duration_hours': 24,
            'initial_cash': 10000.0
        }
    )

    print(f"Created simulation session: {session.session_id}")

    # 4. Run simulation with checkpoints
    sim = MarketSimulation(
        ticker=ticker,
        session_id=session.session_id,
        agents=selected_models,
        checkpoint_manager=CheckpointManager()
    )

    results = sim.run()

    # 5. Store final results
    sim_repo.store_results(session.session_id, results)

    print(f"[{datetime.now()}] Simulation complete!")
    print(f"Winner: {results['winner']}")
    print(f"Top 3:")
    for i, agent in enumerate(results['leaderboard'][:3], 1):
        print(f"  {i}. {agent['agent']}: ROI={agent['roi']:.2f}%, Sharpe={agent['sharpe_ratio']:.2f}")

if __name__ == "__main__":
    # Run immediately
    run_daily_simulation()

    # Or schedule for daily execution
    # schedule.every().day.at("09:00").do(run_daily_simulation)
    # while True:
    #     schedule.run_pending()
    #     time.sleep(60)
```

---

## Phase 7: Updated File Structure

```
/Users/jaiminpatel/github/ai-trader-battlefield/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                        # Flask API (UPDATE: new endpoints)
â”‚   â”œâ”€â”€ main.py                       # Simulation orchestrator (UPDATE: checkpoints)
â”‚   â”œâ”€â”€ daily_runner.py               # NEW: Daily automated runner
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                     # NEW: Database layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ db_config.py              # Connection configuration
â”‚   â”‚   â”œâ”€â”€ models.py                 # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ market_data_repository.py
â”‚   â”‚   â”œâ”€â”€ simulation_repository.py
â”‚   â”‚   â””â”€â”€ checkpoint_repository.py
â”‚   â”‚
â”‚   â”œâ”€â”€ market/
â”‚   â”‚   â”œâ”€â”€ market_simulation.py      # UPDATE: Remove order book, add checkpoints
â”‚   â”‚   â”œâ”€â”€ order_book.py             # DEPRECATE (no longer used)
â”‚   â”‚   â”œâ”€â”€ tick_generator.py         # KEEP (still useful for testing)
â”‚   â”‚   â”œâ”€â”€ finnhub_provider.py       # NEW: Finnhub integration
â”‚   â”‚   â”œâ”€â”€ checkpoint_manager.py     # NEW: 3-hour checkpoint logic
â”‚   â”‚   â”œâ”€â”€ performance_metrics.py    # NEW: Advanced metrics
â”‚   â”‚   â””â”€â”€ agent.py                  # UPDATE: Simplified for open market
â”‚   â”‚
â”‚   â”œâ”€â”€ open_router/
â”‚   â”‚   â”œâ”€â”€ algo_gen.py               # KEEP: Initial algorithm generation
â”‚   â”‚   â”œâ”€â”€ adaptive_algo_gen.py      # NEW: Checkpoint-based re-generation
â”‚   â”‚   â”œâ”€â”€ model_fecthing.py
â”‚   â”‚   â””â”€â”€ ai_agents.json
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         # KEEP for testing, not used in production
â”‚   â”‚   â””â”€â”€ *.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ migrations/                   # NEW: Alembic migrations
â”‚   â”‚   â””â”€â”€ versions/
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt              # UPDATE: Add new dependencies
â”‚   â”œâ”€â”€ .env                          # UPDATE: Add Finnhub key, DB URL
â”‚   â””â”€â”€ README.md                     # UPDATE: New architecture docs
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.js          # UPDATE: New UI for checkpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ CheckpointTimeline.js # NEW: Visualize 3-hour checkpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ MetricsDisplay.js     # NEW: Advanced metrics display
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ docker-compose.yml                # UPDATE: Add PostgreSQL service
â”œâ”€â”€ CLAUDE.md                         # CREATED
â”œâ”€â”€ OPTIMIZATION_PLAN.md              # THIS FILE
â””â”€â”€ README.md                         # UPDATE: New features
```

---

## Phase 8: Implementation Roadmap

### Week 1: Database Foundation
- [ ] Set up PostgreSQL database
- [ ] Create schema (tables for market_data, sessions, checkpoints, trades)
- [ ] Implement repository pattern (market_data_repository.py, etc.)
- [ ] Write Alembic migrations
- [ ] Test database connectivity

### Week 2: Finnhub Integration
- [ ] Sign up for Finnhub API
- [ ] Implement FinnhubDataProvider class
- [ ] Test fetching previous day data
- [ ] Implement data storage in database
- [ ] Handle edge cases (weekends, holidays, API limits)

### Week 3: Remove Order Book, Simplify Execution
- [ ] Modify market_simulation.py to remove order book logic
- [ ] Implement instant open market execution
- [ ] Update agent.py for simplified order format
- [ ] Remove order book dependencies
- [ ] Test with sample agents

### Week 4: Checkpoint System
- [ ] Implement CheckpointManager class
- [ ] Add checkpoint detection to simulation loop
- [ ] Implement checkpoint evaluation logic
- [ ] Store checkpoint data in database
- [ ] Test 3-hour interval detection

### Week 5: Adaptive Algorithm Generation
- [ ] Implement adaptive_algo_gen.py
- [ ] Create prompt templates with performance feedback
- [ ] Test algorithm re-generation at checkpoints
- [ ] Validate generated code before execution
- [ ] Handle generation failures gracefully

### Week 6: Advanced Metrics
- [ ] Implement PerformanceMetrics class
- [ ] Add Sharpe ratio calculation
- [ ] Add max drawdown calculation
- [ ] Add volatility and Sortino ratio
- [ ] Integrate metrics into final results

### Week 7: Daily Automation
- [ ] Implement daily_runner.py
- [ ] Set up cron job or scheduler
- [ ] Test end-to-end daily workflow
- [ ] Implement error handling and logging
- [ ] Add email/Slack notifications

### Week 8: Frontend Updates
- [ ] Create CheckpointTimeline component
- [ ] Create MetricsDisplay component
- [ ] Update Dashboard with new features
- [ ] Add real-time checkpoint visualization
- [ ] Polish UI/UX

### Week 9: Testing & Optimization
- [ ] Write unit tests for new modules
- [ ] Integration testing
- [ ] Performance optimization
- [ ] Database indexing
- [ ] API rate limit handling

### Week 10: Documentation & Deployment
- [ ] Update README.md
- [ ] Update CLAUDE.md
- [ ] Write deployment guide
- [ ] Docker Compose configuration
- [ ] Production deployment

---

## Dependencies Update

**backend/requirements.txt:**
```
# Existing
pandas
numpy
yfinance
matplotlib
requests
gunicorn
python-dotenv
Flask
Flask-Cors

# NEW - Database
psycopg2-binary==2.9.9
SQLAlchemy==2.0.25
alembic==1.13.1

# NEW - Finnhub
finnhub-python==2.4.19

# NEW - Scheduling
schedule==1.2.0

# NEW - Monitoring
sentry-sdk==1.40.0  # Optional: Error tracking
```

**docker-compose.yml UPDATE:**
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: ai_trader_battlefield
      POSTGRES_USER: trader
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  backend:
    build: ./backend
    ports:
      - "5000:5000"
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - FINNHUB_API_KEY=${FINNHUB_API_KEY}
      - DATABASE_URL=postgresql://trader:${DB_PASSWORD}@postgres:5432/ai_trader_battlefield
    depends_on:
      - postgres

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend

volumes:
  postgres_data:
```

---

## Testing Strategy

### Unit Tests
```bash
# Test database repositories
pytest backend/tests/test_repositories.py

# Test Finnhub provider
pytest backend/tests/test_finnhub_provider.py

# Test checkpoint manager
pytest backend/tests/test_checkpoint_manager.py

# Test performance metrics
pytest backend/tests/test_performance_metrics.py
```

### Integration Tests
```bash
# Test full simulation flow
pytest backend/tests/integration/test_simulation_flow.py

# Test checkpoint updates
pytest backend/tests/integration/test_checkpoint_updates.py
```

### End-to-End Tests
```bash
# Run daily simulation with test data
python backend/daily_runner.py --test-mode

# Verify database has correct data
python backend/scripts/verify_data.py
```

---

## Monitoring & Observability

### Logging
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/simulation.log'),
        logging.StreamHandler()
    ]
)
```

### Metrics to Track
- Simulation completion time
- Checkpoint evaluation time
- Algorithm generation time (per model)
- Database query performance
- API rate limit usage (Finnhub, OpenRouter)
- Error rates

### Alerts
- Simulation failure
- Database connection issues
- API rate limit exceeded
- Algorithm generation timeout
- Checkpoint evaluation errors

---

## Cost Estimation

### API Costs (Monthly)
- **Finnhub Free Tier:** 60 calls/min (sufficient for daily fetches)
- **OpenRouter:** ~$0.10-$1.00 per algorithm generation
  - 6 agents Ã— 8 checkpoints Ã— 30 days = 1440 generations/month
  - Estimated: $144-$1440/month (depends on model selection)

### Database Costs
- **Self-hosted PostgreSQL:** Free (Docker)
- **Managed (Heroku/AWS RDS):** $20-$50/month

### Total Monthly Cost: ~$150-$1500 (mostly OpenRouter)

**Cost Optimization:**
- Use cheaper models for intermediate checkpoints
- Use expensive models only for final checkpoints
- Cache algorithm prompts to reduce redundant generations
- Use fallback algorithms if generation fails

---

## Success Metrics

### Technical Success
- âœ… 24-hour simulation completes successfully
- âœ… All 8 checkpoints execute correctly
- âœ… Algorithms update based on performance feedback
- âœ… Advanced metrics calculated accurately
- âœ… Database stores all simulation data

### Business Success
- âœ… Clear winner identified based on composite score
- âœ… Adaptive algorithms outperform static algorithms
- âœ… Sharpe ratio improves over checkpoints
- âœ… Max drawdown stays within acceptable limits
- âœ… System runs reliably daily without manual intervention

---

## Future Enhancements (Post-V1)

### Phase 2 Features
- **Multi-Asset Trading:** Trade multiple stocks simultaneously
- **Portfolio Diversification:** Agents manage portfolio of stocks
- **Risk Management:** Stop-loss, take-profit, position limits
- **Backtesting:** Test algorithms on historical date ranges
- **Live Paper Trading:** Connect to real broker APIs (Alpaca, Interactive Brokers)

### Phase 3 Features
- **Machine Learning Agents:** RL-based agents (PPO, DQN)
- **Sentiment Analysis:** Incorporate news sentiment
- **Options Trading:** Expand to derivatives
- **Multi-Agent Collaboration:** Agents can form teams
- **Tournament Mode:** Leaderboard across multiple days

---

## Conclusion

This optimization plan transforms AI Trader Battlefield from a simple historical simulation to a sophisticated, adaptive trading platform that:

1. âœ… Uses live market data from Finnhub
2. âœ… Executes day-delayed simulations (Day N uses Day N-1 data)
3. âœ… Removes order book complexity (instant open market execution)
4. âœ… Implements 3-hour checkpoints with adaptive algorithm updates
5. âœ… Calculates advanced performance metrics (Sharpe, Max Drawdown, etc.)
6. âœ… Stores all data in a dynamic database
7. âœ… Identifies the best-performing AI model scientifically

**Next Steps:**
1. Review this plan
2. Prioritize phases based on resources
3. Set up development environment (database, API keys)
4. Begin Phase 1 (Database Foundation)
5. Iterate and test incrementally

Good luck with the optimization! ðŸš€ðŸ“ˆ
